Column name changes:

Daily Price Change → Daily_Chg
Future Price Change (7D) → Fut_Chg_7D
Volatility (1D) → Vol_1D
Volatility (7D) → Vol_7D
Added the column Future Price Change (14D) = Fut_Chg_14D / RSI_14, RSI_7

[Integrate the Ordinary Least Squares (OLS) linear regression model in complementarity with polynomial regression. This model will allow us to verify the statistical significance of variables (via p-values) and detect issues such as multicollinearity.]

Subsequently, I used the random forest model to capture non-linear relationships. Unlike linear regression, the random forest is capable of modeling complex interactions between variables. It also helps determine which variables contribute the most to the prediction.
I also used polynomial regression to model non-linear relationships. This method allows for capturing complex curves in the data, which is particularly useful for identifying non-linear trends.

In conclusion, isolating these variations allowed me to discover that the most significant inverse correlations indicate that a high current price, combined with elevated technical indicators (such as moving averages, RSI, or volatility), is associated with a future decline in BTC price. These results support the hypothesis that an overvalued market tends to correct itself in the following weeks.
The random forest and polynomial regression were used to capture complex and non-linear relationships in the data, enabling a better understanding of the factors influencing future price variations.

_________________________________________________________________________________________________________________________________
Random Forest Code:
from sklearn.ensemble import RandomForestRegressor

# Define the variables
X = data_filtered[['NUPL Value', 'BTC price (NUPL)', 'Daily_Chg', 'Vol_1D', 'Vol_7D', 'SMA_7', 'SMA_14', 'RSI_14', 'RSI_7']]
y = data_filtered['Fut_Chg_14D']  # Target variable

# Create and train the model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)

# Feature importance
importance = pd.DataFrame({'Feature': X.columns, 'Importance': rf_model.feature_importances_})
print(importance.sort_values(by='Importance', ascending=False))
__________________________________________________________________________________________________________________________________
Polynomial Regression Code:
from sklearn.preprocessing import StandardScaler

# Add StandardScaler to the pipeline
poly_model = make_pipeline(
    StandardScaler(),
    PolynomialFeatures(degree=2),
    LinearRegression()
)

# Train the model
poly_model.fit(X, y)

# Retrieve the coefficients
poly_features = poly_model.named_steps['polynomialfeatures'].get_feature_names_out(X.columns)
coefficients = poly_model.named_steps['linearregression'].coef_

# Display the coefficients
coefficients_df = pd.DataFrame({'Feature': poly_features, 'Coefficient': coefficients})
print(coefficients_df.sort_values(by='Coefficient', ascending=False))
